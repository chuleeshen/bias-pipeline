{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf9cf149-b950-45ee-ae93-8c2157d28a19",
   "metadata": {},
   "source": [
    "# Keyword Bias Generation\n",
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a542191d-20e4-4b40-b878-c8c27f5e5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m pip install openai\n",
    "!{sys.executable} -m spacy download en\n",
    "!{sys.executable} -m pip install --upgrade diffusers[torch] -q\n",
    "!{sys.executable} -m pip install -U autotrain-advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f86a6-bc35-4dff-9bb9-bb768ff7bf90",
   "metadata": {},
   "source": [
    "### Get prompt keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ea8e7d9-9b96-4734-9925-32682314e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def retrieve_keywords(doc):\n",
    "    keywords = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'VERB', 'PROPN', 'ADJ'}:\n",
    "            keywords.append(token.text)\n",
    "        elif token.ent_type_ in {'NORP'}:\n",
    "            keywords.append(token.text)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa29249-31bd-4506-bd4f-a092e1ad78ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A female Malaysian is eating.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec8a811-7f1e-4fcc-b8cc-8cfb12e180e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female', 'Malaysian', 'eating']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(prompt)\n",
    "keywords = retrieve_keywords(doc)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c54d8-8b74-47fe-9fae-908df3517f29",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get biases related to prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7163ec6-591c-4a5b-a8ad-b8628ae84cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-jJJVpWQAtoKpzNCOUr4F83gSgGUFX9lH5a4UjoItYnI3Ylv5\",\n",
    "    base_url=\"https://api.chatanywhere.tech/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dbeea06-d862-44fd-acc1-1e37022bfcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_35_api(messages: list):\n",
    "    completion = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f0d1206-a176-4715-9789-ae2c96ebd9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### Instructions ###\\nYour task is to provide common biases related to the given keywords and say nothing else.\\n\\nOutput only the keyword and its associated biases where each bias is separated with commas, as shown in the format below.\\n\\n### Format ###\\nkeyword 1: bias 1, bias 2, bias 3 ...\\nkeyword 2: bias 1, bias 2, bias 3 ...\\nkeyword 3: bias 1, bias 2, bias 3 ...\\n\\n### Keywords ###\\nfemale, Malaysian, eating\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Few shot prompting\n",
    "bias_inst = \"\"\"\n",
    "### Instructions ###\n",
    "Your task is to provide common biases related to the given keywords and say nothing else.\n",
    "\n",
    "Output only the keyword and its associated biases where each bias is separated with commas, as shown in the format below.\n",
    "\n",
    "### Format ###\n",
    "keyword 1: bias 1, bias 2, bias 3 ...\n",
    "keyword 2: bias 1, bias 2, bias 3 ...\n",
    "keyword 3: bias 1, bias 2, bias 3 ...\n",
    "\n",
    "### Keywords ###\n",
    "{keywords}\n",
    "\"\"\".format(keywords=', '.join(keywords))\n",
    "bias_inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c89cc81-bb04-40f4-9c9d-caf16ac9d613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female: gender bias, stereotype threat, confirmation bias  \\nMalaysian: ethnic bias, cultural bias, stereotype threat  \\neating: food bias, dietary bias, confirmation bias'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [{'role': 'user','content': bias_inst}]\n",
    "result = gpt_35_api(messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2feb9c7-5f39-42da-a30b-64d029759e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'female': ['gender bias', 'stereotype threat', 'confirmation bias'],\n",
       " 'Malaysian': ['ethnic bias', 'cultural bias', 'stereotype threat'],\n",
       " 'eating': ['food bias', 'dietary bias', 'confirmation bias']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert2Dict(inp):\n",
    "    entries = inp.split('\\n')\n",
    "    result_dict = {}\n",
    "    for entry in entries:\n",
    "        key, values = entry.split(': ')\n",
    "        values_list = [value.strip() for value in values.split(',')]\n",
    "        result_dict[key] = values_list\n",
    "    return result_dict\n",
    "\n",
    "key_bias = convert2Dict(result)\n",
    "key_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce41dd2-6036-47b4-a977-7ad8e56ade09",
   "metadata": {},
   "source": [
    "# Input Prompt Images\n",
    "### Set up SDXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a30ca1d7-1dec-45d7-b668-78307794595d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100 80GB PCIe MIG 3g.40gb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9a6be18-2066-42bf-afd7-8a4afaf384bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lchu0039/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline, AutoencoderKL\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"madebyollin/sdxl-vae-fp16-fix\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipe.to(\"cuda\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e86683-25f8-43df-ae97-a18074b6f1df",
   "metadata": {},
   "source": [
    "### Generate 10 images from the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec4262-1116-44bf-8d63-4d5e29ad2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(prompt=prompt, num_inference_steps=25, num_images_per_prompt = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b13cb-173b-41d0-bc39-a6cf135a8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./gen_img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e54e7de-b847-4a80-9826-26326d3deef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "def image_grid(imgs, rows, cols, resize=256):\n",
    "    assert len(imgs) == rows * cols\n",
    "\n",
    "    if resize is not None:\n",
    "        imgs = [img.resize((resize, resize)) for img in imgs]\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid_w, grid_h = cols * w, rows * h\n",
    "    grid = Image.new(\"RGB\", size=(grid_w, grid_h))\n",
    "\n",
    "    # Check if save_path exists\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        img.save(f\"{save_path}/{i}.png\")\n",
    "        x = i % cols * w\n",
    "        y = i // cols * h\n",
    "        grid.paste(img, box=(x, y))\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56886f74-8cdc-4c16-a86b-b2ea4b1e79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(image.images, 2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4d390-764f-4194-acca-833f9c626cc9",
   "metadata": {},
   "source": [
    "# Generated Images VQA\n",
    "### Set up MiniGPT-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d4767-826b-45f4-887c-cd22fe4a44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import html\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torchvision.transforms as T\n",
    "from collections import defaultdict\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.registry import registry\n",
    "from minigpt4.conversation.conversation import Conversation, SeparatorStyle, Chat\n",
    "import torch.backends.cudnn as cudnn\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc47c60-2475-4008-be96-044c1f7d0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8c74c-00f9-4710-bea8-f1ff4054569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and model loading\n",
    "def initialize_model(config_path, gpu_id):\n",
    "    args = argparse.Namespace(cfg_path=config_path, gpu_id=gpu_id, options=None)\n",
    "    print(args)\n",
    "    cfg = Config(args)\n",
    "    device = f'cuda:{args.gpu_id}'\n",
    "    model_config = cfg.model_cfg\n",
    "    model_config.device_8bit = args.gpu_id\n",
    "    model_cls = registry.get_model_class(model_config.arch)\n",
    "    model = model_cls.from_config(model_config).to(device)\n",
    "    model = model.eval()\n",
    "    vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "    vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "    return model, vis_processor, device\n",
    "\n",
    "# Upload and process image\n",
    "def process_image(image_path, vis_processor):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = vis_processor(image).unsqueeze(0).to(device)\n",
    "    return image_tensor\n",
    "\n",
    "# Provide a prompt to return output\n",
    "def ask_question(model, vis_processor, device, image_tensor, prompt):\n",
    "    chat = Chat(model, vis_processor, device=device)\n",
    "    chat_state = Conversation(\n",
    "        system=\"\",\n",
    "        roles=(r\"<s>[INST] \", r\" [/INST]\"),\n",
    "        messages=[],\n",
    "        offset=2,\n",
    "        sep_style=SeparatorStyle.SINGLE,\n",
    "        sep=\"\",\n",
    "    )\n",
    "    img_list = [image_tensor]\n",
    "\n",
    "    chat.upload_img(image_tensor, chat_state, img_list)\n",
    "    chat.ask(prompt, chat_state)\n",
    "\n",
    "    llm_message = chat.answer(conv=chat_state, img_list=img_list, temperature=0.6, max_new_tokens=500, max_length=2000)[0]\n",
    "    return llm_message\n",
    "\n",
    "# Main function to use the above utilities\n",
    "def main(image_path, prompt, config_path='./eval_configs/minigptv2_eval.yaml', gpu_id=0):\n",
    "    model, vis_processor, device = initialize_model(config_path, gpu_id)\n",
    "    image_tensor = process_image(image_path, vis_processor)\n",
    "    answer = ask_question(model, vis_processor, device, image_tensor, prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8defa9-4fc6-4805-867f-60bbb2fdbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_image_path = \"./gen_img/0.png\"\n",
    "prompt = \"Describe this image in detail.\"\n",
    "output = main(gen_image_path, prompt)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b3185f-aee2-45da-a37c-591abe148c76",
   "metadata": {},
   "source": [
    "### Set up MiniGPT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642d093-005f-41ea-a875-bcd6735366dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import StoppingCriteriaList\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.dist_utils import get_rank\n",
    "from minigpt4.common.registry import registry\n",
    "from minigpt4.conversation.conversation import Chat, CONV_VISION_Vicuna0, CONV_VISION_LLama2, StoppingCriteriaSub\n",
    "\n",
    "# imports modules for registration\n",
    "from minigpt4.datasets.builders import *\n",
    "from minigpt4.models import *\n",
    "from minigpt4.processors import *\n",
    "from minigpt4.runners import *\n",
    "from minigpt4.tasks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce599ec-6621-4ca9-8056-7196d4307336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse arguments\n",
    "def parse_args(args_list):\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "    parser.add_argument(\"--cfg-path\", required=True, help=\"path to configuration file.\")\n",
    "    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\n",
    "        \"--options\",\n",
    "        nargs=\"+\",\n",
    "        help=\"override some settings in the used config, the key-value pair \"\n",
    "        \"in xxx=yyy format will be merged into config file (deprecate), \"\n",
    "        \"change to --cfg-options instead.\",\n",
    "    )\n",
    "    args = parser.parse_args(args_list)\n",
    "    return args\n",
    "\n",
    "# Function to set up seeds\n",
    "def setup_seeds(config):\n",
    "    seed = config.run_cfg.seed + get_rank()\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "# Function to initialize the model\n",
    "def initialize_model(cfg_path, gpu_id):\n",
    "    args = parse_args(['--cfg-path', cfg_path, '--gpu-id', str(gpu_id)])\n",
    "    cfg = Config(args)\n",
    "    \n",
    "    setup_seeds(cfg)\n",
    "    \n",
    "    model_config = cfg.model_cfg\n",
    "    model_config.device_8bit = args.gpu_id\n",
    "    model_cls = registry.get_model_class(model_config.arch)\n",
    "    model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "    \n",
    "    conv_dict = {'pretrain_vicuna0': CONV_VISION_Vicuna0, 'pretrain_llama2': CONV_VISION_LLama2}\n",
    "    CONV_VISION = conv_dict[model_config.model_type]\n",
    "    \n",
    "    vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "    vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "    \n",
    "    stop_words_ids = [[835], [2277, 29937]]\n",
    "    stop_words_ids = [torch.tensor(ids).to(device='cuda:{}'.format(args.gpu_id)) for ids in stop_words_ids]\n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "    \n",
    "    chat = Chat(model, vis_processor, device='cuda:{}'.format(args.gpu_id), stopping_criteria=stopping_criteria)\n",
    "    \n",
    "    return chat, CONV_VISION, vis_processor\n",
    "\n",
    "# Function to process image and prompt\n",
    "def process_image_and_prompt(chat, CONV_VISION, vis_processor, image_path, prompt):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    chat_state = CONV_VISION.copy()\n",
    "    img_list = []\n",
    "    llm_message = chat.upload_img(img, chat_state, img_list)\n",
    "    chat.encode_img(img_list)\n",
    "    chat.ask(prompt, chat_state)\n",
    "    response = chat.answer(conv=chat_state,\n",
    "                           img_list=img_list,\n",
    "                           num_beams=1,\n",
    "                           temperature=1.0,\n",
    "                           max_new_tokens=300,\n",
    "                           max_length=2000)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aad705-339b-42dd-a220-e006b60cdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model (provide your config path and GPU ID here)\n",
    "cfg_path = \"/ibm/gpfs/home/lchu0039/eval_configs/minigpt4_eval.yaml\"\n",
    "gpu_id = 0  # change if necessary\n",
    "chat, CONV_VISION, vis_processor = initialize_model(cfg_path, gpu_id)\n",
    "\n",
    "# Process an image and prompt (provide your image path and prompt here)\n",
    "gen_image_path = \"./gen_img/0.png\"\n",
    "prompt = \"Describe this image in detail.\"\n",
    "response = process_image_and_prompt(chat, CONV_VISION, vis_processor, image_path, prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf4f745-0bf5-485d-9704-13c7ec929c32",
   "metadata": {},
   "source": [
    "### Set up LLaMA 3 with vision capabilities utilising SIGLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c645e7-5d60-4dd2-a6aa-e7e3b53fe91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!{sys.executable} -m pip install --upgrade torch transformers pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9e052-57f2-4634-be15-1a892da9b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_skip_modules=[\"mm_projector\", \"vision_model\"],\n",
    ")\n",
    "\n",
    "model_path = \"./llama-3-vision-alpha-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_cfg,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    use_fast=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37224903-f72a-43d4-bfec-030acaa3ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few shot prompting\n",
    "phrase_inst = \"\"\"\n",
    "### Instructions ###\n",
    "Your task is to provide the phrases from the given sentence related to the given topic and say nothing else.\n",
    "\n",
    "Output only the phrases where each phrase is separated with slashes, as shown in the format below.\n",
    "\n",
    "### Format ###\n",
    "phrase 1 / phrase 2 / phrase 3 ...\n",
    "\n",
    "### Sentence ###\n",
    "{sentence}\n",
    "\n",
    "### Topic ###\n",
    "{topic}\n",
    "\"\"\"\n",
    "\n",
    "def generate_captions(prompt, topic): \n",
    "  phrase_col = []\n",
    "  for i in range(10):\n",
    "    gen_image_path = \"./gen_img/\" + str(i) + \".png\"\n",
    "    image = Image.open(gen_image_path)\n",
    "\n",
    "    desc_output = tokenizer.decode(model.answer_question(image, prompt, tokenizer), skip_special_tokens=True)\n",
    "    # print(str(i) + \": \" + desc_output)\n",
    "    \n",
    "    mod_inst = phrase_inst.format(sentence=desc_output, topic=topic)\n",
    "    \n",
    "    messages = [{'role': 'user','content': mod_inst}]\n",
    "    result = gpt_35_api(messages)\n",
    "    phrase_col.append((i, result))\n",
    "  return phrase_col\n",
    "\n",
    "def show_phrases(arr):\n",
    "  for i in range(len(arr)):\n",
    "    print(\"Image \" + str(i) + \": \")\n",
    "    print(output[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708fdff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Describe the image\"\n",
    "topic = 'gender'\n",
    "output = generate_captions(prompt, topic)\n",
    "show_phrases(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff0d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'gender bias'\n",
    "output = generate_captions(prompt, topic)\n",
    "show_phrases(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b32da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'food preference'\n",
    "output = generate_captions(prompt, topic)\n",
    "show_phrases(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'food preference bias'\n",
    "output = generate_captions(prompt, topic)\n",
    "show_phrases(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f616aa63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
